import os
from openai import OpenAI
import base64
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

def ask_gpt_vision(question, image_path):
    with open(image_path, "rb") as f:
        b64_image = base64.b64encode(f.read()).decode("utf-8")
    image_url = f"data:image/jpeg;base64,{b64_image}"

    completion = client.chat.completions.create(
        model="qwen-vl-max-latest",  # å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹
        messages=[
            {
                "role": "system",
                "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚è¯·ç›´æ¥ç”¨ç®€æ˜ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦ä»»ä½• Markdown æˆ–æ ¼å¼åŒ–ç¬¦å·ã€‚æ¯æ¬¡è¾“å‡ºå°½é‡æ§åˆ¶åœ¨20ä¸ªæ±‰å­—å·¦å³ï¼Œé€‚åˆè¯­éŸ³å’Œæ§åˆ¶å°ç›´æ¥æ’­æŠ¥ã€‚"}],
            },
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": question},
                ],
            },
        ],
        max_tokens=500,
    )
    return completion.choices[0].message.content

def ask_gpt_vision_stream(question, image_path):
    with open(image_path, "rb") as f:
        b64_image = base64.b64encode(f.read()).decode("utf-8")
    image_url = f"data:image/jpeg;base64,{b64_image}"

    stream = client.chat.completions.create(
        model="qwen-vl-max-latest",  # å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹
        messages=[
            {
                "role": "system",
                "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚è¯·ç›´æ¥ç”¨ç®€æ˜ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦ä»»ä½• Markdown æˆ–æ ¼å¼åŒ–ç¬¦å·ã€‚æ¯æ¬¡è¾“å‡ºå°½é‡æ§åˆ¶åœ¨20ä¸ªæ±‰å­—å·¦å³ï¼Œé€‚åˆè¯­éŸ³å’Œæ§åˆ¶å°ç›´æ¥æ’­æŠ¥ã€‚"}],
            },
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": question},
                ],
            },
        ],
        max_tokens=500,
        stream=True,
    )
    for chunk in stream:
        delta = chunk.choices[0].delta
        if hasattr(delta, "content") and delta.content:
            yield delta.content

def ask_gpt_text_stream(question):
    stream = client.chat.completions.create(
        model="qwen-plus",  # çº¯æ–‡æœ¬æ¨¡å‹
        messages=[
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„ä¸­æ–‡åŠ©æ‰‹ã€‚è¯·ç›´æ¥ç”¨ç®€æ˜ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦ä»»ä½• Markdown æˆ–æ ¼å¼åŒ–ç¬¦å·ã€‚æ¯æ¬¡è¾“å‡ºå°½é‡æ§åˆ¶åœ¨20ä¸ªæ±‰å­—å·¦å³ï¼Œé€‚åˆè¯­éŸ³å’Œæ§åˆ¶å°ç›´æ¥æ’­æŠ¥ã€‚"}
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question}
                ],
            },
        ],
        max_tokens=500,
        stream=True,
    )
    for chunk in stream:
        delta = chunk.choices[0].delta
        if hasattr(delta, "content") and delta.content:
            yield delta.content

def handle_sentence(question):
    global is_processing
    if is_processing:
        print("AI æ­£åœ¨å›ç­”ï¼Œå¿½ç•¥æœ¬æ¬¡è¾“å…¥ã€‚")
        return
    is_processing = True
    print(f"ğŸ§  is_processing: {is_processing}")
    try:
        if not question:
            is_processing = False
            return
        if "é€€å‡º" in question:
            print("æ£€æµ‹åˆ°é€€å‡ºæŒ‡ä»¤ï¼Œç¨‹åºç»“æŸã€‚")
            exit(0)
        print(f"ğŸ§  ä½ é—®çš„æ˜¯: {question}")

        image_path = None
        if need_camera_by_llm(question):
            image_path = capture_frame()
            print("ğŸ“¤ æ­£åœ¨è¯·æ±‚ é€šä¹‰åƒé—®ï¼ˆå¸¦å›¾ç‰‡ï¼‰")
        else:
            print("ğŸ“¤ æ­£åœ¨è¯·æ±‚ é€šä¹‰åƒé—®ï¼ˆçº¯æ–‡æœ¬ï¼‰")

        if image_path:
            text_stream = ask_gpt_vision_stream(question, image_path)
        else:
            text_stream = ask_gpt_text_stream(question)

        # åªæ‰“å°å†…å®¹ï¼Œä¸è°ƒç”¨TTS
        for part in text_stream:
            print("text_streamå†…å®¹:", repr(part), type(part))
        print()  # æ¢è¡Œ
    finally:
        is_processing = False